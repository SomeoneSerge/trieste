:py:mod:`trieste.models.gpflow.sampler`
=======================================

.. py:module:: trieste.models.gpflow.sampler

.. autoapi-nested-parse::

   This module is the home of the sampling functionality required by Trieste's
   GPflow wrappers.



Module Contents
---------------

.. py:class:: IndependentReparametrizationSampler(sample_size: int, model: trieste.models.interfaces.ProbabilisticModel)

   Bases: :py:obj:`trieste.models.interfaces.ReparametrizationSampler`

   This sampler employs the *reparameterization trick* to approximate samples from a
   :class:`ProbabilisticModel`\ 's predictive distribution as

   .. math:: x \mapsto \mu(x) + \epsilon \sigma(x)

   where :math:`\epsilon \sim \mathcal N (0, 1)` is constant for a given sampler, thus ensuring
   samples form a continuous curve.

   :param sample_size: The number of samples to take at each point. Must be positive.
   :param model: The model to sample from.
   :raise ValueError (or InvalidArgumentError): If ``sample_size`` is not positive.

   .. py:method:: sample(self, at: trieste.types.TensorType, *, jitter: float = DEFAULTS.JITTER) -> trieste.types.TensorType

      Return approximate samples from the `model` specified at :meth:`__init__`. Multiple calls to
      :meth:`sample`, for any given :class:`IndependentReparametrizationSampler` and ``at``, will
      produce the exact same samples. Calls to :meth:`sample` on *different*
      :class:`IndependentReparametrizationSampler` instances will produce different samples.

      :param at: Where to sample the predictive distribution, with shape `[..., 1, D]`, for points
          of dimension `D`.
      :param jitter: The size of the jitter to use when stabilising the Cholesky decomposition of
          the covariance matrix.
      :return: The samples, of shape `[..., S, 1, L]`, where `S` is the `sample_size` and `L` is
          the number of latent model dimensions.
      :raise ValueError (or InvalidArgumentError): If ``at`` has an invalid shape or ``jitter``
          is negative.



.. py:class:: BatchReparametrizationSampler(sample_size: int, model: trieste.models.interfaces.ProbabilisticModel)

   Bases: :py:obj:`trieste.models.interfaces.ReparametrizationSampler`

   This sampler employs the *reparameterization trick* to approximate batches of samples from a
   :class:`ProbabilisticModel`\ 's predictive joint distribution as

   .. math:: x \mapsto \mu(x) + \epsilon L(x)

   where :math:`L` is the Cholesky factor s.t. :math:`LL^T` is the covariance, and
   :math:`\epsilon \sim \mathcal N (0, 1)` is constant for a given sampler, thus ensuring samples
   form a continuous curve.

   :param sample_size: The number of samples for each batch of points. Must be positive.
   :param model: The model to sample from.
   :raise ValueError (or InvalidArgumentError): If ``sample_size`` is not positive.

   .. py:method:: sample(self, at: trieste.types.TensorType, *, jitter: float = DEFAULTS.JITTER) -> trieste.types.TensorType

      Return approximate samples from the `model` specified at :meth:`__init__`. Multiple calls to
      :meth:`sample`, for any given :class:`BatchReparametrizationSampler` and ``at``, will
      produce the exact same samples. Calls to :meth:`sample` on *different*
      :class:`BatchReparametrizationSampler` instances will produce different samples.

      :param at: Batches of query points at which to sample the predictive distribution, with
          shape `[..., B, D]`, for batches of size `B` of points of dimension `D`. Must have a
          consistent batch size across all calls to :meth:`sample` for any given
          :class:`BatchReparametrizationSampler`.
      :param jitter: The size of the jitter to use when stabilising the Cholesky decomposition of
          the covariance matrix.
      :return: The samples, of shape `[..., S, B, L]`, where `S` is the `sample_size`, `B` the
          number of points per batch, and `L` the dimension of the model's predictive
          distribution.
      :raise ValueError (or InvalidArgumentError): If any of the following are true:
          - ``at`` is a scalar.
          - The batch size `B` of ``at`` is not positive.
          - The batch size `B` of ``at`` differs from that of previous calls.
          - ``jitter`` is negative.



.. py:class:: RandomFourierFeatureTrajectorySampler(model: trieste.models.interfaces.ProbabilisticModel, dataset: trieste.data.Dataset, num_features: int = 1000)

   Bases: :py:obj:`trieste.models.interfaces.TrajectorySampler`

   This class builds functions that approximate a trajectory sampled from an underlying Gaussian
   process model. For tractibility, the Gaussian process is approximated with a Bayesian
   Linear model across a set of features sampled from the Fourier feature decomposition of
   the model's kernel. See :cite:`hernandez2014predictive` for details.

   Achieving consistency (ensuring that the same sample draw for all evalutions of a particular
   trajectory function) for exact sample draws from a GP is prohibitively costly because it scales
   cubically with the number of query points. However, finite feature representations can be
   evaluated with constant cost regardless of the required number of queries.

   In particular, we approximate the Gaussian processes' posterior samples as the finite feature
   approximation

   .. math:: \hat{f}(x) = \sum_{i=1}^m \phi_i(x)\theta_i

   where :math:`\phi_i` are m Fourier features and :math:`\theta_i` are
   feature weights sampled from a posterior distribution that depends on the feature values at the
   model's datapoints.

   Our implementation follows :cite:`hernandez2014predictive`, with our calculations
   differing slightly depending on properties of the problem. In particular,  we used different
   calculation strategies depending on the number of considered features m and the number
   of data points n.

   If :math:`m<n` then we follow Appendix A of :cite:`hernandez2014predictive` and calculate the
   posterior distribution for :math:`\theta` following their Bayesian linear regression motivation,
   i.e. the computation revolves around an O(m^3)  inversion of a design matrix.

   If :math:`n<m` then we use the kernel trick to recast computation to revolve around an O(n^3)
   inversion of a gram matrix. As well as being more efficient in early BO
   steps (where :math:`n<m`), this second computation method allows much larger choices
   of m (as required to approximate very flexible kernels).

   :param sample_size: The desired number of samples.
   :param model: The model to sample from.
   :param dataset: The data from the observer. Must be populated.
   :sample_min_value: If True then sample from the minimum value of the function,
       else sample the function's minimiser.
   :param num_features: The number of features used to approximate the kernel. We use a default
       of 1000 as it typically perfoms well for a wide range of kernels. Note that very smooth
       kernels (e.g. RBF) can be well-approximated with fewer features.
   :raise ValueError: If ``dataset`` is empty.

   .. py:method:: get_trajectory(self) -> trieste.models.interfaces.TrajectoryFunction

      Generate an approximate function draw (trajectory) by sampling weights
      and evaluating the feature functions.

      :return: A trajectory function representing an approximate trajectory from the Gaussian
          process, taking an input of shape `[N, D]` and returning shape `[N, 1]`



